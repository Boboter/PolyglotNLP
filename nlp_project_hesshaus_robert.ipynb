{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155e61b4-2f31-49ac-a1d1-d2e88b2d77f6",
   "metadata": {},
   "source": [
    "# NLP Project: Language Detection with spaCy\n",
    "\n",
    "This Notebook documents the process of creating my first language detection model, leveraging datasets encompassing three languages: German, Spanish, and English, with the plan to add more.\n",
    "\n",
    "The datasets for this project are sourced from [Tatoeba](https://tatoeba.org/en/downloads), a repository that offers an extensive array of sentences across numerous languages, with weekly updates to ensure richness and diversity. The project aims to build a robus model capable of identifying the aforementioned languages.\n",
    "\n",
    "Author: Robert Heßhaus  \n",
    "Date: [19/03/2024](date:\"dmy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa44db1-85cc-4019-83c9-e775d2a3070b",
   "metadata": {},
   "source": [
    "## Loading Packages\n",
    "\n",
    "Let's start with loading the required libraries and set up the environment for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb72b09-d51d-47e7-b1a2-f1f6bf185f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de import German\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_de = German()\n",
    "nlp_es = Spanish()\n",
    "nlp_en = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b8733-db0d-4f61-b059-7d4729602f53",
   "metadata": {},
   "source": [
    "## Loading Language Files\n",
    "\n",
    "Next up I'll load the language files and merge them into one to make it more accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143fca51-a24c-4879-888c-88ac897f600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "\n",
    "def load_data(file):\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['ID', 'Language', 'Sentence'])\n",
    "    return df\n",
    "\n",
    "# Paths to Language Files\n",
    "\n",
    "en_file = \"./lang_data/eng_sentences.tsv\"\n",
    "de_file = \"./lang_data/deu_sentences.tsv\"\n",
    "es_file = \"./lang_data/spa_sentences.tsv\"\n",
    "\n",
    "# Load Data\n",
    "\n",
    "df_en = load_data(en_file)\n",
    "df_de = load_data(de_file)\n",
    "df_es = load_data(es_file)\n",
    "\n",
    "# Merge into one\n",
    "\n",
    "df_combined = pd.concat([df_en, df_de, df_es]).reset_index(drop=True)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5f29d-953e-4ad3-8a68-123ba7e939d3",
   "metadata": {},
   "source": [
    "## Checking and Removal of Duplicates\n",
    "\n",
    "Eventhough Tatoeba regularly checks and updates their datasets, we still want to make sure to remove all duplicate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b48b1e1-9eb1-4e61-bae1-88d5100390d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.drop_duplicates(subset=\"Sentence\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922697f8-c835-41ec-a4ff-3c31f18bdbec",
   "metadata": {},
   "source": [
    "## Normalziation and Tokenization\n",
    "\n",
    "To prepare the data for model training, normalization and tokenization are crucial steps. These processes involve converting the text to a uniform format and breaking it down into manageable pieces (tokens), respectively. For this purpose, we leverage the spaCy library, which offers robust tools for natural language processing across multiple languages.\r\n",
    "\r\n",
    "Normalization ensures consistency in text representation by converting all characters to lowercase and removing non-language-specific characters. Tokenization then splits the text into individual words or symbols, allowing for more effective language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03bfb356-06cd-4af9-901f-c79339b08140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Language</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Normalized_Sentence</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11101026</td>\n",
       "      <td>spa</td>\n",
       "      <td>Este es mi ordenador portable.</td>\n",
       "      <td>este es mi ordenador portable</td>\n",
       "      <td>[este, es, mi, ordenador, portable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5813019</td>\n",
       "      <td>deu</td>\n",
       "      <td>Tom ist bei uns zu Hause nicht gerne gesehen.</td>\n",
       "      <td>tom ist bei uns zu hause nicht gerne gesehen</td>\n",
       "      <td>[tom, ist, bei, uns, zu, hause, nicht, gerne, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9425561</td>\n",
       "      <td>eng</td>\n",
       "      <td>Nobody knows why Tom did it.</td>\n",
       "      <td>nobody knows why tom did it</td>\n",
       "      <td>[nobody, knows, why, tom, did, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8567236</td>\n",
       "      <td>eng</td>\n",
       "      <td>This towel has a nasty odor.</td>\n",
       "      <td>this towel has a nasty odor</td>\n",
       "      <td>[this, towel, has, a, nasty, odor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3075050</td>\n",
       "      <td>eng</td>\n",
       "      <td>Minetest is a clone of Minecraft.</td>\n",
       "      <td>minetest is a clone of minecraft</td>\n",
       "      <td>[minetest, is, a, clone, of, minecraft]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Language                                       Sentence  \\\n",
       "0  11101026      spa                 Este es mi ordenador portable.   \n",
       "1   5813019      deu  Tom ist bei uns zu Hause nicht gerne gesehen.   \n",
       "2   9425561      eng                   Nobody knows why Tom did it.   \n",
       "3   8567236      eng                   This towel has a nasty odor.   \n",
       "4   3075050      eng              Minetest is a clone of Minecraft.   \n",
       "\n",
       "                             Normalized_Sentence  \\\n",
       "0                 este es mi ordenador portable    \n",
       "1  tom ist bei uns zu hause nicht gerne gesehen    \n",
       "2                   nobody knows why tom did it    \n",
       "3                   this towel has a nasty odor    \n",
       "4              minetest is a clone of minecraft    \n",
       "\n",
       "                                              Tokens  \n",
       "0                [este, es, mi, ordenador, portable]  \n",
       "1  [tom, ist, bei, uns, zu, hause, nicht, gerne, ...  \n",
       "2                 [nobody, knows, why, tom, did, it]  \n",
       "3                 [this, towel, has, a, nasty, odor]  \n",
       "4            [minetest, is, a, clone, of, minecraft]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the text\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert Text to  lowercase    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removal of all special letters not relevant to the languages\n",
    "    text = re.sub(r\"[^a-zäöüßáéíóúñÁÉÍÓÚÑ]+\", ' ', text) \n",
    "    return text\n",
    "\n",
    "# Tokenize the Rows based on Language\n",
    "def tokenize_with_language(row):\n",
    "    text = row['Normalized_Sentence']\n",
    "    lang = row['Language']\n",
    "\n",
    "    if not text:\n",
    "        return []\n",
    "    # Select the appropriate spaCy model based on language\n",
    "    if lang == 'deu':\n",
    "        doc = nlp_de(text)\n",
    "    elif lang == 'eng':\n",
    "        doc = nlp_en(text)\n",
    "    elif lang == 'spa':\n",
    "        doc = nlp_es(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized language code: {lang}\")\n",
    "    \n",
    "    # Extract the tokens from the processed row\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Shuffling the Data to ensure random distribution\n",
    "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Apply normalization and tokenization\n",
    "df_combined['Normalized_Sentence'] = df_combined['Sentence'].apply(normalize_text)\n",
    "df_combined['Tokens'] = df_combined.apply(tokenize_with_language, axis=1)\n",
    "\n",
    "# Display the frst few rows of processed data\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0e0e3-3aa4-4d43-bf89-84a019fe348e",
   "metadata": {},
   "source": [
    "## Vectorization & Model compilation\n",
    "\n",
    "To train our neural network model, the first step is to convert the preprocessed textual data into a numerical format, a process known as vectorization. For this task, we employ the TfidfVectorizer from the scikit-learn library, which transforms the text into a TF-IDF (Term Frequency-Inverse Document Frequency) matrix. This method highlights the importance of each word in the context of its sentence and across the dataset.\r\n",
    "\r\n",
    "Following the vectorization, the data is utilized to train a Naive Bayes model. Despite being a simpler classification algorithm compared to neural networks, Naive Bayes is remarkably effective for text classification tasks, including language detection, due to its assumption of independence among predictor\n",
    "\n",
    "For further details, visit:\n",
    "- [TFidfVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- [Naive Bayes Classifier on WIkipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e71ff62-9b9e-4aee-b627-94525b221fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.72%\n"
     ]
    }
   ],
   "source": [
    "# Extract sentences and labels\n",
    "sentences = df_combined['Normalized_Sentence']\n",
    "labels = df_combined['Language']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2)\n",
    "\n",
    "# Initialization and training of the vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions using the classifier and calculate accuracy\n",
    "predictions = classifier.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82017441-08f3-4fa3-9b35-29d2868a91b2",
   "metadata": {},
   "source": [
    "## Testing Lab\n",
    "\n",
    "To try out the model, write the sentences like the Example Sentence in the array \"new_sentences\" below and then run the cell.\\\n",
    "If an error shows up, make sure you ran all code cells above this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e580d3-0a05-4c1f-a4fd-5738c24fbcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'This is an example Sentence'\n",
      "Predicted Language: English\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\n",
    "    \"This is an example Sentence\"\n",
    "]\n",
    "normalized_sentences = [normalize_text(sentence) for sentence in new_sentences]\n",
    "new_sentences_tfidf = vectorizer.transform(normalized_sentences)\n",
    "predictions = classifier.predict(new_sentences_tfidf)\n",
    "\n",
    "\n",
    "language_codes = {'deu': 'German', 'eng': 'English', 'spa': 'Spanish'}\n",
    "predicted_languages = [language_codes[code] for code in predictions]\n",
    "\n",
    "for sentence, prediction in zip(new_sentences, predicted_languages):\n",
    "    print(f\"Sentence: '{sentence}'\\nPredicted Language: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d03ec-c3cb-4942-844d-5b4f139c9b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
